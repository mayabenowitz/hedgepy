{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Financial Market Dislocations with Causal-Inference and Anomaly Detection: A Quantamental Approach\n",
    "<br />\n",
    "\n",
    "\n",
    "## Motivation\n",
    "<br />\n",
    "\n",
    "Funds that fail to beat their benchmarks quickly go extinct. Generating alpha is an extraordinarily difficult task. One that is becoming increasingly difficult, a reality illustrated by deteriorating hedge fund returns. **Find alpha or die** is the stark new reality for every portfolio manager. At the same time, we are living through a technology driven data explosion. In the world’s 2.5 billion gigabytes of data, Wall Street sees its savior. The prevailing belief is that this data — and the predictive power it promises — is the most powerful alpha source to emerge in the last quarter century. The $3 trillion hedge fund industry is currently\n",
    "betting its future on it. This sentiment is especially true as markets enter a period of unprecedented volatility and uncertainty as the world grapples with the fallout of the pandemic, growing social and financial inequality, and now widespread civil unrest. A market once thought \"wacky\" has tipped into full-blown madness as the world seemingly unravels before our eyes. To this end, I am proposing a quantamental approach to help us navigate current and future crises. \n",
    "\n",
    "<br />\n",
    "\n",
    "To be clear: models are not magic, they have their limitations and are only as good as their assumptions and the quality of the data they ingest. Every model, regardless of complexity, should under-go sufficient levels of scrutiny. Models may appear predictive when they are not and can lose predictive power as market conditions change. It is therefore in the firm's best interest for us to reserve belief in the model until it passes the appropriate checks-and-balances. In the remaining sections of this notebook, I do my best to take this \"black box\" and make it as transparent as possible without losing the forest for the trees (pun intended).\n",
    "\n",
    "<br />\n",
    "\n",
    "With oceans of information, and only so much time to investigate fundamentals before pulling the trigger on an investment decision, where we look for opportunities and risk is of critical importance. It is becoming increasingly easy to lose the forest through the trees, especially on particularly demanding days. Running a lean shop has its disadvantages and this is one of them. While for the foreseeable future, machines are unlikely to master the fundamentals, they are particularly good at sifting through large amounts of data and finding patterns otherwise invisible to the analyst. Models can provide clues the analyst can not see and the analyst can provide intuition and expertise the models can not comprehend. The idea is this: build a model that detects anomalous movements in pairwise correlations of market indices that most often precede market dislocations. The anomalies offer an executable signal that is ultimately at the discretion of individual desks to further investigate and act upon. In the following sections of this notebook, I walk the reader through the model, the assumptions I make, the promising preliminary results I obtain, and the conclusions I make.\n",
    "\n",
    "<br />\n",
    "\n",
    "## Market Sociology\n",
    "<br />\n",
    "\n",
    "Suppose we survey every investor on the planet. Assuming everyone has access to the same N pieces of information, we could ask: given the nth piece, are you a bear or a bull? Market behavior is often considered to reflect external economic news, though empirical evidence has challenged this connection [1,2]. Indeed, it is ultimately the investor's internal outlook and biases that determine how they answer the question. They can imagine a threat when there is none and ignore one when there is. What is more, investors can and often will, ignore accumulating evidence of an economic crisis — until they don't — and panic.\n",
    "\n",
    "<br />\n",
    "\n",
    "> *In sociology [3–5], panic has been defined as a collective flight from a real or imagined threat. In economics, bank runs occur at least in part because of the risk to the individual from the bank run itself—and may be triggered by predisposing conditions, external (perhaps catastrophic) events, or even randomly [6, 7]. Although empirical studies of panic are difficult, efforts to distinguish endogenously (self-generated) and exogenous (externally-generated) market panics from oscillations of market indices have met with some success [8–10], though the conclusions have been debated [11–14]. The literature generally uses the volatility and the correlation between asset prices to characterize risk [15–19]. These measures are sensitive to the magnitude of price movement and therefore increase dramatically when there is a market crash.* - [D. Harmon, M. Aguiar, D. Chinellato, Predicting Economic Market Crises using Measures of Collective Panic, ArXiv (2011)](https://arxiv.org/pdf/1102.2620.pdf)\n",
    "\n",
    "<br />\n",
    "\n",
    "This proposal is not radically different from what has been achieved in the literature. By making precise measurements of correlations between asset prices and the volatility of those correlations, we can nowcast a more complete picture of the market and look for early warning signals of extreme volatility and market dislocations. The reality of this approach is that correlations are non-stationary (they are notoriously unstable) and are known to harbor non-linear effects [20]. As such, instead of using Pearson's correlation, I use Székely's correlation, which measures both linear and non-linear associations in the data.\n",
    "\n",
    "1. [D. Cutler, J. Poterba, and L. Summers, What Moves Stock Prices? Journal of Portfolio Managment, 15, 4 (1989).]( https://dspace.mit.edu/bitstream/handle/1721.1/64351/whatmovesstockpr00cutl.pdf?sequence=1&isAllowed=y)\n",
    "2. [D. Harmon, M. Aguiar, D. Chinellato, Predicting Economic Market Crises using Measures of Collective Panic, ArXiv (2011)](https://arxiv.org/pdf/1102.2620.pdf)\n",
    "3. [E. L. Quarantelli, The Sociology of Panic, in International Encyclopedia of the Social and Behavioral Sciences, N. J. Smelser and P. B. Baltes, Eds., (Elsevier, 2001)](http://www.csap.cam.ac.uk/media/uploads/files/1/mawson-2005-emotional-attachments.pdf)\n",
    "4. [N. J. Smelser, Theory of Collective Behavior. (Free Press, Glencoe, Ill., 1963)](https://www.researchgate.net/publication/257770268_Looking_at_Smelser's_Theory_of_Collective_Behavior_After_Almost_50_Years_A_Review_and_Appreciation)\n",
    "5. [A. Mawson, Understanding Mass Panic and Other Collective Responses to Threat and Disaster, Psychology, Medicine, Psychiatry (2005)](https://www.semanticscholar.org/paper/Understanding-Mass-Panic-and-Other-Collective-to-Mawson/c4df2c5209a8772b17f4652dc6518627c2465649)\n",
    "6. [D.W. Diamond, P.H. Dybvig, Fed. Res. Bank. Minn. Quart. Rev. 24, 14 (2000).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.9916&rep=rep1&type=pdf)\n",
    "7. [C. W. Calomiris, G. Gorton, in Financial Markets and Financial Crises R. G. Hubbard, Ed. (National Bureau of Economic Research, 1990), chap. 4.](https://www.nber.org/books/glen91-1)\n",
    "8. [J. K. Galbraith, The Great Crash 1929 (Houghton Mifflin, New York, 1954).](https://scholarlycommons.law.case.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=3598&context=caselrev)\n",
    "9. [C. Kindleberger, Manias, panics, and crashes (John Wiley & Sons Inc., New York, 1978).](http://vvernon.sunyempirefaculty.net/newperspectives/wp-content/uploads/2015/08/ManiasPanicsandCrashes.pdf)\n",
    "10. [D. Sornette, A. Johansen, J.-P. Bouchaud, Journal of Physics I (France) 6, 167 (1996).](https://arxiv.org/pdf/cond-mat/9510036.pdf)\n",
    "11. [J. A. Feigenbaum, P.G.O. Freund, Int. J. Mod. Phys. B 10, 3737 (1996).](https://arxiv.org/pdf/cond-mat/9509033.pdf)\n",
    "12. [D. Sornette, A. Johansen, Physica A 245, 411 (1997).](https://arxiv.org/pdf/cond-mat/9704127;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Time Series of Asset Correlation Networks\n",
    "\n",
    "A simple mathematical representation of a collection of financial assets is a weighted [graph](ipedia.org/wiki/Graph_(discrete_mathematics)) (otherwise known as a network). Intutively, a graph captures the relations between objects -- abstract or concrete. Formally, a weighted graph is an ordered tuple $G = (V, E, W)$ where $V$ is a set of *verticies* (or nodes), $E$ is the set of pairwise relationships between the vertices (the *edges*), and $W$ is a set of numerical values assigned to each edge.\n",
    "\n",
    "<br />\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/9/9a/Weighted_network.png)\n",
    "<center>A weighted graph with ten vertices and twelve edges.</center>\n",
    "\n",
    "<br />\n",
    "\n",
    "A useful representation of $G$ is the *adjacency matrix*:\n",
    "<center>$$\n",
    "A_{ij} = \\begin{cases} \n",
    "      1, & \\text{if} \\ i \\ \\text{is adjacent to} \\ j \\  \\\\\n",
    "      0, & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "$$</center>\n",
    "\n",
    "<br />\n",
    "\n",
    "Here the pairwise relations are expressed as the $ij$ entries of an $N \\times N$ matrix, where $N$ is the number of nodes (assets). In what follows, the adjacency matrix becomes a critical instrument of our anomaly detection algorithm. **The strategy is to transform the time-dependent feature vectors (e.g., the daily opening, closing, high, and low prices) into a time series of graphs, with edges weighted by the daily correlations between each pair of feature vectors.** This method is closely related to a time-varying version of the work presented by [Tse, *et al*. (2010)](http://cktse.eie.polyu.edu.hk/pdf-paper/JoEF-1009.pdf). (See [Stock Correlation Network](https://en.wikipedia.org/wiki/Stock_correlation_network) for a summary.)  Once the data is in this form, we apply [graph centrality measures](https://towardsdatascience.com/graph-analytics-introduction-and-concepts-of-centrality-8f5543b55de3), averaged over the entire graph, to obtain signals we test for predictive power, and use as the basis of our anomaly detection algorithm.\n",
    "\n",
    "<br />\n",
    "\n",
    "## What on Earth is Székely's correlation and Why Should We Care?\n",
    "\n",
    "Put simply, [Székely's correlation](https://en.wikipedia.org/wiki/Distance_correlation#cite_note-SR2007-2) is a generalization of Pearson's correlation insofar as it (1) detects both linear and non-linear associations in the data and (2) can be applied to time series of unequal dimension. Below is a comparison of Székely's and Pearson's correlation.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/400px-Correlation_examples2.svg.png)\n",
    "<center>Pearson's correlation coefficients of sample data</center>\n",
    "\n",
    "<br/><br/>\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Distance_Correlation_Examples.svg/400px-Distance_Correlation_Examples.svg.png)\n",
    "<center>Székely's correlation coefficients of sample data</center>\n",
    "\n",
    "<br />\n",
    "\n",
    "Székely's correlation varies between 0 and 1. A Székely's correlation close to 0 indicates a pair of time series is **independent** where values close to 1 indicate a high degree of **dependence**. This is in contrast to Pearson's correlation, which varies between -1 and 1, and can be 0 for dependent time series (see [Szekely, *et al*. (2017)](https://arxiv.org/pdf/0803.4101.pdf)). What makes Székely's correlation particularly appealing is the fact that it can be applied to time series of unequal dimension. This property allows us to compute the correlation between feature vectors with missing data.\n",
    "\n",
    "<br />\n",
    "\n",
    "### Mathematical Details\n",
    "For the mathematically inclined reader, I give a brief overview of the derivation of Székely's correlation. Let $(X_{k}, Y_{k})$, &thinsp; $k = 1,2, ... , n$ be a statistical sampling of a pair of real-valued or vector-valued random variables. First, compute the $n \\times n$ distance matrices \n",
    "\n",
    "\\begin{equation}\n",
    "A_{j,k} = ||X_{j} - X_{k}||, \\ \\ j,k=1,2,...,n\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "B_{j,k} = ||Y_{j} - Y_{k}||, \\ \\ j,k=1,2,...,n\n",
    "\\end{equation}\n",
    "\n",
    "<br />\n",
    "where ||.|| denotes the Euclidean norm. Then take the $n \\times n$ centering matrix\n",
    "\n",
    "\\begin{equation}\n",
    "C_{n} = I_{n} - \\dfrac{1}{n}\\mathbb{O}\n",
    "\\end{equation}\n",
    "\n",
    "<br />\n",
    "where $I_{n}$ is the identity matrix and $\\mathbb{O}$ is an $n \\times n$ matrix of all 1's, and doubly center both distance matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{A} = C_{n}AC_{n}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{B} = C_{n}BC_{n}\n",
    "\\end{equation}\n",
    "\n",
    "<br />\n",
    "where both the row and column means are equal to zero. Next, calculate the sample distance covariance and the sample distance variance\n",
    "<br/><br/>\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{dCov}_{n}(X, Y) = \\dfrac{1}{n}\\sqrt{\\sum_{j=1}^{n}\\sum_{k=1}^{n}A_{j,k}B_{j,k}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{dVar}_{n}(X) := \\text{dCov}_{n}(X,X) = \\dfrac{1}{n}\\sqrt{\\sum_{k,l}A^{2}_{k,l}}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, Székely's correlation (a.k.a the Distance correlation) is \n",
    "<br/><br/>\n",
    "\\begin{equation}\n",
    "\\text{dCor}(X,Y) = \\dfrac{\\text{dCov}(X,Y)}{\\sqrt{\\text{dVar}(X)\\text{dVar}(Y)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep and Exploratory Analysis\n",
    "<br/>\n",
    "In this section I read in the DJA index (as of April 2020) from 2000-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, coint, adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# import custom library tools\n",
    "from hedgepy.core import build_series, build_network_time_series\n",
    "from hedgepy.utils import write_series, read_series, read_data\n",
    "from hedgepy.centrality import global_degree_centrality, global_eigencentrality\n",
    "\n",
    "# enable importing external modules\n",
    "sys.path.append(sys.path[0] + \"/..\")\n",
    "\n",
    "# autoreload magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-05-11</th>\n",
       "      <td>101.37</td>\n",
       "      <td>104.25</td>\n",
       "      <td>99.00</td>\n",
       "      <td>102.81</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-12</th>\n",
       "      <td>106.00</td>\n",
       "      <td>110.50</td>\n",
       "      <td>104.77</td>\n",
       "      <td>107.62</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-15</th>\n",
       "      <td>108.06</td>\n",
       "      <td>108.06</td>\n",
       "      <td>100.12</td>\n",
       "      <td>101.00</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-16</th>\n",
       "      <td>104.52</td>\n",
       "      <td>109.06</td>\n",
       "      <td>102.75</td>\n",
       "      <td>105.69</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-17</th>\n",
       "      <td>103.62</td>\n",
       "      <td>103.69</td>\n",
       "      <td>100.37</td>\n",
       "      <td>101.37</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open    high     low   close  name\n",
       "date                                            \n",
       "2000-05-11  101.37  104.25   99.00  102.81  AAPL\n",
       "2000-05-12  106.00  110.50  104.77  107.62  AAPL\n",
       "2000-05-15  108.06  108.06  100.12  101.00  AAPL\n",
       "2000-05-16  104.52  109.06  102.75  105.69  AAPL\n",
       "2000-05-17  103.62  103.69  100.37  101.37  AAPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dja = pd.read_csv('../experiments/data/interim/DJA-2000-2020-clean.csv')\n",
    "df_dja['date'] = pd.to_datetime(df_dja['date'])\n",
    "df_dja.set_index('date', inplace=True)\n",
    "df_dja = df_dja.drop('Unnamed: 0', axis=1)\n",
    "df_dja.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_ts_hard = build_network_time_series(dja_series, soft_threshold=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
